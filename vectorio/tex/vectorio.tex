% acm doc
% https://authors.acm.org/proceedings/production-information/preparing-your-article-with-latex

\documentclass[manuscript]{acmart}

% BUILDING. Do not use ``latex''
% USE
%   brew install mactex
%   cd tex
%   pdflatex -shell-escape vectorio.tex
%   bibtex vectorio
%   ...rerun pdflatex, maybe twice

%\usepackage{babel}
\usepackage{graphicx}
\usepackage{color}
%\usepackage{cite}
%\usepackage{algorithmic}
%\usepackage{algorithmicx}
\usepackage[ruled,vlined,boxed]{algorithm2e}
\usepackage{listings}
%\usepackage{minted}
\usepackage{underscore}
\usepackage{multicol}
\usepackage{float}
\usepackage{checkend}
\usepackage{enumitem}


% must come before begin{document}
\citestyle{acmauthoryear}
\setcopyright{none}

\title[Hadoop Vector IO]{Hadoop Vector IO: cloud IO for columnar data formats}

% Yes, this titling is broken
\author{Mukund Thakur}
%  \texttt{mthakur@apache.org}
\author{Steve Loughran}
\author{Owen O'Malley}
%  \texttt{owen@apache.org}
\author{Rajesh Balamohan}

\renewcommand{\shortauthors}{Thakur, et al.}
\date{May 2024}

% ========================================================================

\begin{document}

% ========================================================================

\begin{abstract}
The Hadoop Filesystem API has been extended to support scatter/gather IO
for columnar data formats.
This paper describes the design and implementation of this feature,
and the integration with Apache ORC and Parquet, and
presents results from TPC-DS benchmarks.

Or key findings are that
\begin{itemize}
  \item An asynchronous scatter/gather IO API is ideally suited to retrieval
  of ``stripes''/``row groups'' of data stored in columnar data formats.
  \item Java's native IO APIs support such an API, offering speedups when reading
  local data.
  \item Against cloud storage, the ability to issue parallel GET requests
        significantly improves read performance, providing tangible performance
        improvements to queries which read large quantities of data.
  \item Using the API in the ORC and Parquet is sufficient to enable
        speedups in applications using the libraries \emph{without} any
        changes in the application code.
  \item The API shows that "cloud first" APIs provide tangible speedups
        against the classic Posix API. Many more opportunities exist to
        provide and exploit such APIs.
\end{itemize}

\end{abstract}



\maketitle

% ========================================================================

\section{Introduction}
\label{sec:introduction}



\subsection{Terminology}\label{subsec:terminology}

First, some terminology needs to be introduced to describe
the protocols.

% ========================================================================

\section{The Challenge of Object Stores}
\label{sec:object-stores}

Having introduced the classic filesystem and the commit protocols and algorithms
used to commit the output of distributed computation, let us consider
Object Stores such as Amazon S3, Google Cloud Storage and
Windows Azure Storage.

% As all filesystem
%operations are via the NameNode, all clients get a consistent view of the filesystem.
%And, as the


The most salient point, is this: Object Stores are not filesystems.
Rather than the classic hierarchical view of directories, subdirectories
and paths, object stores store a set of objects, each with a unique key;
a sequence of characters provided when the object was created.
Classic path separators ``\texttt{/}'' are invariably part of the set of valid
characters, so allowing objects to be created which have the appearance
of files in a directory.

As examples, the following are all valid keys on the Amazon, Google and Microsoft
stores

\begin{verbatim}
/entry
/path1/path2/path3
/path1/
/path1
\end{verbatim}

More subtly, it is valid for an object store container (on S3:, a ``bucket'')
to have objects with all of these names simultaneously.
It is not an error to have an object whose key would make it appear to be
``under'' another object, nor to explicitly contain path entries separators.

Objects cannot generally be appended to once created, or renamed.
They can be replaced by new objects or deleted.
Some form of copy operation permits an object to be duplicated, creating
a new object with a different key.
Such copy operations take place within the storage infrastructure with a
copy time measurable in megabytes/second.


The set of operations offered are normally an extended set of HTTP verbs:

\begin{description}[leftmargin=8em, style=nextline]
  \item[PUT] Atomic write of an object
  \item[GET] retrieve all or part of an object
  \item[HEAD] retrieve the object metadata
  \item[LIST] list all objects starting with a given prefix
  \item[COPY] copy a single object within the store, possibly from other containers.
  \item[DELETE] Delete an object
\end{description}

The java bigdata ecosystem has been built around the Hadoop Filesystem API,
which is a sub-POSIX API for reading and writing files.
Unlike POSIX, it is not possible to write anywhere but the end of a file,
and appending an existing file may not be supported.

For reading files, as well as the traditional read() and seek() operations,
the Hadoop API has a PositionedReadable interface, which allows for

\begin{verbatim}
public interface PositionedReadable {
  int read(long position, byte[] buffer, int offset, int length);
  void readFully(long position, byte[] buffer, int offset, int length);
  void readFully(long position, byte[] buffer) throws IOException;
}
\end{verbatim}

The \texttt{readFully()} operations are important in that they
request an exact number of bytes from a specific position and offset,
raising a java exception if the range is beyond the end of the file.

In cloud stores, these operations can be optimized as the exact range is known;
if data is read in blocks, all required blocks can be requested as
a single GET request, or multiple requests in parallel.

Alternatively, the store may simply issue a single GET request for the data.

This \texttt{PositionedReadable} interface is the primary interface
used by file formats such as ORC and Parquet to read data, as it matches
their use: read specific ranges of a file into on-heap memory.

For this reason, effort has been invested it in optimizing the
cloud store implementations.
However, all these suffer from latency: the offset is not known until
the call is made, and as the applications read columns at non-seqential
locations in the files, prefetching is of little or no benefit.


\section{Strategies for improving performance}\label{sec:strategies-for-improving-performance}

What options are there for speeding up file read performance?

\subsection{Prefetching}\label{subsec:prefetching}

The Azure ABFS connector has a small pool of prefetching threads
which will fetch blocks from open input streams ahead of
their current read position.

We have some empirical evidence that prefetched blocks are not used
in columnar data based on an ABFS prefetching bug\ \cite{HADOOP-18521}.
Prefetched blocks could be corrupted -yet this was never observed when
processing Parquet/ORC data, implying that these blocks were not read.
It was discovered reading CSV of many gigabytes in size, and
in testing could be replicated reading CSV, JSON and Avro files,
all of which are row-based formats reading large parts of the file
sequentially.

PInterest, however, did observe tangible speedups in their Parquet workloads
through their prefetching input stream\ \cite{Bhalchandra:01}.

\begin{quotation}
 A standalone benchmark showed a 12x improvement in S3 read throughput
 (from 21 MB/s to 269 MB/s).
 Increased throughput allowed our production jobs to finish sooner.
 As a result, we saw 22\% reduction in vcore-hours, 23\% reduction in memory-hours,
 and similar reduction in run time of a typical production job.
\end{quotation}

Examining the results in more detail, its clear that block prefetching
itself was not the cause of the speedup, \emph{it was caching of fetched
and prefetched blocks}.

\begin{quotation}
Parquet files require non-sequential access as dictated by their on-disk format.
Our initial implementation did not use a local cache.
Each time there was a seek outside of the current block, we had to discard any prefetched data.
That resulted in worse performance compared to the stock reader when it came to reading from Parquet files.

We observed significant improvement in the read throughput for Parquet files once we
introduced the local caching of prefetched data.
Currently, our implementation increases Parquet file reading throughput by 5x compared to the stock
reader.
\end{quotation}

This code has now been the Hadoop S3A connector\ \cite{HADOOP-18028},
work which was done in parallel with the work described in this paper.
They have not yet been fully merged -though our goal is switch to the
PInterest stream once this is done, we have confidence in the code
and selected a fetch/cache strategy for our own work which is suitable
for most workloads.

\subsection{Footer Caching}\label{subsec:footer-caching}

Parquet and ORC files have variable length footers the end of the file which contain
information about the file.

\begin{verbatim}

    4-byte magic number "PAR1" or "PARE"
    <Column 1 Chunk 1 + Column Metadata>
    <Column 2 Chunk 1 + Column Metadata>
    ...
    <Column N Chunk 1 + Column Metadata>
    <Column 1 Chunk 2 + Column Metadata>
    <Column 2 Chunk 2 + Column Metadata>
    ...
    <Column N Chunk 2 + Column Metadata>
    ...
    <Column 1 Chunk M + Column Metadata>
    <Column 2 Chunk M + Column Metadata>
    ...
    <Column N Chunk M + Column Metadata>
    File Metadata
    4-byte length in bytes of file metadata (little endian)
    4-byte magic number "PAR1" or "PARE"
\end{verbatim}

For Parquet, the metadata is summarized in~\ref{sec:parquet-metadata}.
The mey part of the metadata includes not just the schema of
the columns, but the details about where each column's data is stored
in the file.

As it this footer is of variable length, unless the location is already known
to the application/worker process, the first steps of opening a file
is the sequence of

\begin{enumerate}
  \item{open the file (HEAD request, unless file length is declared)}
  \item{read the final 8 bytes to validate filetype and the offset for the "real" footer: GET request}
  \item{read the full footer: GET request unless locally cached}
\end{enumerate}

The operation 3 in this sequence is interpeted in the S3A connector as a sign
that the application will be performing random IO in file and rather than do full-length GET requests,
it should switch to ranged GET requests, where the range is determine from the read operations.

The google GCS connector has long supported caching of file footers.
We do not have any data on the speedups this provides.

The ABFS connector added` this feature in 2024;
\ \cite{HADOOP-18971}; it is not yet in a public release and
again we do not have any statistics on the speedups it provides.


\subsection{Parallel fetching of column stripes}\label{subsec:parallel-fetching-of-column-stripes}

This is the solution we implemented and which this paper describes.



% ========================================================================

\section{Design}
\label{sec:design}

The design of the vector IO API had some core requirements:
\begin{itemize}
  \item Support scatter/gather IO through local filesystem through the java NIO API.
  \item Local reads to support with CRC validation of data read.
  \item Efficient support against cloud stores where parallel GET requests are required.
  \item Support scatter/gather IO through HDFS and other distributed filesystems
        where the POSIX API and Hadoop PositionedReadable interface are available
  \item Optimize for reading columnar data formats such as ORC and Parquet
  \item Be easy to integrate with the ORC and Parquet libraries, in the open
        source releases, as well as our own internal branches.
  \item Permit out-of-order result processing, even while the current format libraries
        do not yet support this.
\end{itemize}

What is key is: it \emph{must} be possible to use the vector IO API against
any existing Hadoop input stream, with the base implementation
falling back to the existing \texttt{PositionedReadable} readFully() method,
with the option of custom higher performance implementations --- including
for the local filesystem as well as cloud storage.

Java NIO API:

extending PositionedReadable with range coalescing

Async API with back-references for wiring up.

S3A Impl as ranged GET requests with (limited) parallelism.

% ========================================================================

\section{Integration with ORC and Parquet}
\label{sec:integration}

It was our belief that integration with the ORC and Parquet libraries was
sufficient to deliver tangible speedups
\subsection{ORC}\label{subsec:orc}

\subsection{Parquet}\label{subsec:parquet}


% ========================================================================

\section{Results}
\label{sec:results}

\subsection{External tests}\label{subsec:external-tests}

\begin{quotation}
 Some numbers from an independent benchmark.
 I used Spark to parallelize the reading of all rowgroups (just the reading of the raw data)
 from TPC-DS/SF10000/store_sales using various APIs.

I used a modified (lots of stuff removed) version of the ParquetFileReader and a custom
benchmark program that reads all the row groups in parallel and records the time spent in each read from S3.
The modified version of ParquetFileReader can switch between the various methods of reading from S3.
The entry AWS SDK V2 is a near copy of the Iceberg S3FileIO code though.

\end{quotation}

\begin{verbatim}
32 executors, 16 cores
fs.s3a.threads.max = 20
\end{verbatim}

\begin{table}
  \begin{tabular}{ l l l l }
    \hline
    \textbf{Reader} & \textbf{Mean Time/min)} & \textbf{Median} & \textbf{Baseline}\\
    Parquet	& 10.32	& 10 & 1 \\
    Parquet Vector IO	& 2.02 & 2	& 5.1 \\
    SDK V2 & 9.86 & 10 & 1 \\
    SDK V2 Async & 9.66	& 9.6 & 1.1 \\
    SDK V2 AsyncCrt	& 9.76 & 10 & 1.1 \\
    SDK V2 S3TransferManager & 9.58 & 9.5 & 1.1 \\
    SDK V2 Async CRT Http Client & 10.8 & 11 & 1 \\
    \hline
  \end{tabular}
\caption{Independent benchmark results}
\label{tab:independent-benchmark-results}
\end{table}

\begin{quotation}

I saw issues with the CRT client when running at scale causing JVM crashes.
And the V2 transfer manager did not do range reads properly.

Summary - The various V2 SDK clients provide lower latency and better upload speeds but for raw data scans,
they are all pretty much the same.
Increasing the parallelism as vector IO does, has maximum benefit.
\end{quotation}

Analysis.

This benchmark was done with a Hadoop 3.3.x release which had not yet moved to
the AWS SDK V2; this is why the baseline 10:32 time is slower than the "AWS SDK V2"
timing.
Now that Hadoop 3.4.x has moved to the AWS SDK V2, we would expect the v2 timings to
be the default, so the speedups from other methods would not be quite so pronounced.

The AWS V2 SDK includes an "asynchronous client" which is only used by the S3A
connector when copying or uploading files, not when reading them.
The S3TransferManager is the class used to manage these operations.
The "CRT http client" is a native C library to which the SDK can delegate much
of the work of building, signing, sending and receiving HTTP requests, and processing
the responses.

Although the CRT promises speedups, our experience with using openssl instead of
the Java JRE's open TLS implementation makes us wary of the potential for
unusual failure conditions which may occur in some deployments.

For the open source hadoop releases, moving to the "Async" client is something
which could be done with lower risk; switching to CRT library would then be
an optional which could be explicitly enabled in deployments.

% ========================================================================

\section{Limitations}
\label{sec:limitations}


% ========================================================================
\section{Other performance enhancements}
\label{sec:enhancements}

\subsection{Footer prefetching and caching}\label{subsec:footer-prefetching-and-caching}

As shown in\ \cite{zeng2023empirical} and in our own analyisis of trace data
obtained from audit logs of storage systems, reading the footer date of ORC and Parquet
files can be a significant cost.

When these files are first opened, the initial read sequence is

\begin{itemize}
  \item{seek(EOF-8)}
  \item{read the final 8 bytes to validate filetype and the length for the "real" footer}
  \item{seek(EOF-offset)}
  \item{read (offset..EOF-8)}
\end{itemize}

\subsubsection{Footer caching in filesystem client}

The GCS and ABFS connectors can both be configured to cache footers.

For this they need to expect to be reading parquet or ORC files, and be configured
with a footer cache size.

Once this is done, it is transparent to the application that the data has been fetched.

If the cache is large enough, this saves a GET request; if too small then
a second must be issued -which will no more expensive than the original uncached
implementation.

However, if this caching is enabled for all files, then there is a cost
of a superflous GET request for files which are not Parquet or ORC files.
This is a literal cost, and can reduce the IO capacity of the storage system.

\subsubsection{Coalescing footer reads in the ORC/Parquet libraries}
If an "expected footer size" is known, the two reads can be coalesced into a single read

\begin{itemize}
  \item{seek(EOF-expected)}
  \item{block = read[](expected..EOF)}
  \item{scan block[len-8..len] bytes to validate filetype and the offset for the "real" footer}
  \item{process block[len - offset..len-8)}
\end{itemize}

If the block read is greater than the expected footer size, then the single read
is sufficient, saving one network round trip, and only reading the extra bytes of
\texttt{block.len-(footer + 8)}.

If not enough bytes were read, then extra data can be requested, but it is a more complex
operation to combine the two reads.

This pulls the prefetching logic into the library.
The benefit is: the library knows its own format, and all stores will gain from the feature.
The cost is that the library now has a slightly more complex read operation,
including tests for all conditions, especially the case where the footer is larger
than the expected data.

\subsubsection{Schema propagation across processes}

Apache Tez passes the schema of the columns being read from the driver to
the executors (todo: check this).
This avoids the need to parse it again.

If the footer size of each file was already known (how?), then it could be
attached too.


\subsubsection{File length and offset retentin in manifest files}

Tables being stored in cloud datalakes are moving towards manifest-based indices over structures inferred by directory trees.
If the manifest file includes the length of each file and the offset where the
real footer begins, then the initial HEAD on a file open can be saved,
and footer read can be optimized knowing the actual range, rather
than guessed/manually configured.

While this may seem a marginal gain, it is a tangible overhead when reading
small files, and is a needless part of the read capacity of a cloud store.



\subsubsection{cloud stores: Multi-ranged GET requests}.

The HTTP protocol allows for a client to request multiple ranges of a file
in the same request.
If supported by cloud object stores, this will allow for a single request
over a single HTTP connection to retrieve different row groups in the same
request, \emph{irrespecive of their location in the file}.
This would assist coalescing range requests, reduce the number of HTTP requests
needed while avoiding the need to discard data between ranges.



\section{Related Work}\label{sec:related-work}

\section{Conclusions and Further Work}\label{sec:conclusions-and-further-work}

Our results show that adding support for vectored reads into the API used to
access cloud-store-hosted data, can deliver significant performance
on queries which read large quantities of Parquet and ORC data.

As these formats, Parquet in particular, are the foundational formats in cloud-hosted
data lakes\footnote{We are ignoring CSV files. Please, stop it.},
this means that this should reduce the query time of many queries
made of this data.

If there is a penalty for this, it is that it is somewhat more
complicated to use ---a problem exacerbated by the need to use
Java Reflection in the public implementation.

Fortunately, only two libraries need to be upgraded to support this,
the Java implementations of Parquet and ORC\@.
All applications have to do is
\begin{itemize}
    \item Use a version of the library with the feature
    \item Run the application with a version of Hadoop client with the feature
    \item Enable vector reads for the library.
\end{itemize}
That's all.

Where next?

Most pressing is adding the same support to Azure ADLS Gen 2 client, the
``abfs connector''\ \cite{HADOOP-18884}.
This will permit us to benchmark performance against a
input stream with optional block prefetching,
which is needed for a broader comparison.

For Parquet, we have some immediate improvements we wish
to do, to pick up existing performance enhancements
which again, a desire to support older Hadoop releases
requires their use to be through Java Reflection.
There is ongoing work to aid this\ \cite{HADOOP-19131},
after which Parquet will be able to pass down the
file length when opening a file -removing a HEAD request
on every operation.
While seemingly marginal, for small files this can be a factor.

More ambitious would be for the parquet to be provided with the footer offset
within the file --eliminating the need for the initial 8 byte read.
This becomes possible with the move to manifest-based table indices;
if these manifests declared file length and footer position then we
could use that when opening files -at which point multiple HEAD and GET
requests can be eliminated.

One outstanding research topic would be to explore how the Parquet/ORC
libraries and the query engines using them could take advantage of
the fact the results are now potentially returned out of order.

Currently the calling thread will block until all the results of all ranges have
been retrieved, at which point processing the results begin.
This is a straightforward strategy, leads to simple code, and is effective
if the time spent waiting for data is neglible.
When running Hive or Spark queries against data stored on the same host as
the code itself is running, these times are (probably) neglible.
When the application is working with data in cloud storage, however,
this no longer holds.
Given the ranges are being returned in the order the GET requests complete,
it may be possible for the Parquet library to perform the next stage
of the data processing, pruning based on pushed-down predicates, while
subsequent data arrives.
As such, there \emph{may} be performance improvements from asynchronous
result processing.
The Vector IO API was designed to support this.
Asynchronous processing of cloud-based data retrieved through vector IO does not, however, guarantee
speedups.
Why not?
Because the data is being received as TLS-encrypted HTTP responses,
which requires CPU time to process, it is not clear that the
cores ``executing'' the query are idle; instead they may be
busy reading in the data, decrypting it and saving to the final
output buffers.
If so, reworking a library for asynchronous range processing may
be a major undertaking for little or no benefit.

And yet: the API is not exclusively for cloud storage; native file
reading permits DMA copying of data from HDD or SSD storage
directly into the target buffers, while changes in
server design keep IO processing an area of
competitive innovation~\cite{10.1145/3620665.3640401}.



% ========================================================================

\begin{acks}
We are grateful for the contributions of all reviewers and testers of this work,
especially our fellow developers in the open source community,
especially Dongjoon Hyun, Parth Chandra, and Gang Wu.
Special mention must be made of the Cloudera QE team whose scale testing
not only produced the numbers in this paper, but also identified a large number
of regressions related unrelated changes in the S3A codebase (move to AWS v2 SDK).
As much of that work was contributed by Mukund and Steve, we are grateful.
\end{acks}


% ========================================================================

\section{References}
\label{sec:references}

% Bibliography. Include

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
\bibliography{../citations/main}
\bibliography{../citations/format}
\bibliography{../citations/db}



\appendix{}

\section{Parquet Metadata Structure}
\label{sec:parquet-metadata}

The footer metadata is described in Parquet's thrift schema; here
are selected parts of this.
\begin{verbatim}
/**
 * Description for file metadata
 */
struct FileMetaData {
  /** Version of this file **/
  1: required i32 version

  /** Parquet schema for this file.  This schema contains metadata for all the columns.
   * The schema is represented as a tree with a single root.  The nodes of the tree
   * are flattened to a list by doing a depth-first traversal.
   * The column metadata contains the path in the schema for that column which can be
   * used to map columns to nodes in the schema.
   * The first element is the root **/
  2: required list<SchemaElement> schema;

  /** Number of rows in this file **/
  3: required i64 num_rows

  /** Row groups in this file **/
  4: required list<RowGroup> row_groups

  /** Optional key/value metadata **/
  5: optional list<KeyValue> key_value_metadata

  6: optional string created_by

  /**
   * Sort order used for the min_value and max_value fields in the Statistics
   * objects and the min_values and max_values fields in the ColumnIndex
   * objects of each column in this file.
   */
  7: optional list<ColumnOrder> column_orders;/
  8: optional EncryptionAlgorithm encryption_algorithm
  9: optional binary footer_signing_key_metadata
}

/** Crypto metadata for files with encrypted footer **/
struct FileCryptoMetaData {

  1: required EncryptionAlgorithm encryption_algorithm
  2: optional binary key_metadata
}

struct RowGroup {
  /** Metadata for each column chunk in this row group.
   * This list must have the same order as the SchemaElement list in FileMetaData.
   **/
  1: required list<ColumnChunk> columns

  /** Total byte size of all the uncompressed column data in this row group **/
  2: required i64 total_byte_size

  /** Number of rows in this row group **/
  3: required i64 num_rows

  /** If set, specifies a sort ordering of the rows in this RowGroup.
   * The sorting columns can be a subset of all the columns.
   */
  4: optional list<SortingColumn> sorting_columns

  /** Byte offset from beginning of file to first page (data or dictionary)
   * in this row group **/
  5: optional i64 file_offset

  /** Total byte size of all compressed (and potentially encrypted) column data
   *  in this row group **/
  6: optional i64 total_compressed_size

  /** Row group ordinal in the file **/
  7: optional i16 ordinal
}



struct ColumnChunk {
  /** File where column data is stored.  If not set, assumed to be same file as
    * metadata.  This path is relative to the current file.
    **/
  1: optional string file_path

  /** Byte offset in file_path to the ColumnMetaData **/
  2: required i64 file_offset

  /** Column metadata for this chunk. This is the same content as what is at
   * file_path/file_offset.  Having it here has it replicated in the file
   * metadata.
   **/
  3: optional ColumnMetaData meta_data

  /** File offset of ColumnChunk's OffsetIndex **/
  4: optional i64 offset_index_offset

  /** Size of ColumnChunk's OffsetIndex, in bytes **/
  5: optional i32 offset_index_length

  /** File offset of ColumnChunk's ColumnIndex **/
  6: optional i64 column_index_offset

  /** Size of ColumnChunk's ColumnIndex, in bytes **/
  7: optional i32 column_index_length

  /** Crypto metadata of encrypted columns **/
  8: optional ColumnCryptoMetaData crypto_metadata

  /** Encrypted column metadata for this chunk **/
  9: optional binary encrypted_column_metadata
}

**
 * Represents a element inside a schema definition.
 *  - if it is a group (inner node) then type is undefined and num_children is defined
 *  - if it is a primitive type (leaf) then type is defined and num_children is undefined
 * the nodes are listed in depth first traversal order.
 */
struct SchemaElement {
  /** Data type for this field. Not set if the current element is a non-leaf node */
  1: optional Type type;

  /** If type is FIXED_LEN_BYTE_ARRAY, this is the byte length of the values.
   * Otherwise, if specified, this is the maximum bit length to store any of the values.
   * (e.g. a low cardinality INT col could have this set to 3).  Note that this is
   * in the schema, and therefore fixed for the entire file.
   */
  2: optional i32 type_length;

  /** repetition of the field. The root of the schema does not have a repetition_type.
   * All other nodes must have one */
  3: optional FieldRepetitionType repetition_type;

  /** Name of the field in the schema */
  4: required string name;

  /** Nested fields.  Since thrift does not support nested fields,
   * the nesting is flattened to a single list by a depth-first traversal.
   * The children count is used to construct the nested relationship.
   * This field is not set when the element is a primitive type
   */
  5: optional i32 num_children;

  /**
   * DEPRECATED: When the schema is the result of a conversion from another model.
   * Used to record the original type to help with cross conversion.
   *
   * This is superseded by logicalType.
   */
  6: optional ConvertedType converted_type;

  /**
   * DEPRECATED: Used when this column contains decimal data.
   * See the DECIMAL converted type for more details.
   *
   * This is superseded by using the DecimalType annotation in logicalType.
   */
  7: optional i32 scale
  8: optional i32 precision

  /** When the original schema supports field ids, this will save the
   * original field id in the parquet schema
   */
  9: optional i32 field_id;

  /**
   * The logical type of this SchemaElement
   *
   * LogicalType replaces ConvertedType, but ConvertedType is still required
   * for some logical types to ensure forward-compatibility in format v1.
   */
  10: optional LogicalType logicalType
}


\end{verbatim}

\section{Postmortem}\label{sec:postmortem}
We like to introspect and review what problems actually occurred
in production.
This helps us review our assumptions, consider how valid they proved
to be, and to let us analyze how they got through our reviews and testing.
Hopefully others can learn from our mistakes.


\subsection{HADOOP-19101. Vectored Read into off-heap buffer broken in fallback implementation}
\label{subsec:hadoop-19101}

Default implementation reads into Direct (off JVM-heap buffer reads broken.
This surfaced when adding tests for azure storage.
Our previous tests against the local and S3 stores used custom implementations
which were correct.
Lesson: always test those defaults.
This has never been seen ``in the wild'' because the applications
using the library do not seem to be reading into Direct buffers.
As well as fixing the bug, the integration patch for Parquet reports all
attempt to use vectored IO with Direct buffers as ``unsupported''.
This ensures that even when using old releases, the bug will not surface.

This issue was discovered while improving range checking as requested by
the reviewers of the Parquet PR.
This shows the value in code review: not only can it find bugs in
the source code being reviewed, it can also find bugs in the code being
invoked.

\end{document}
